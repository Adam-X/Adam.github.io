<!--
 * @Author: Kai Li
 * @Date: 2021-06-10 22:22:59
 * @LastEditors: Please set LastEditors
 * @LastEditTime: 2022-06-19 02:20:18
-->
<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Super-resolution, Deep Learning, Convolutional Neural Network, Computer Vision" />
    <title>Kai Li</title>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
    <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Sans+Pro:300,300i,600">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,300,400italic' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Roboto" />
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
    <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
    <script src="https://code.jquery.com/jquery-2.2.1.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

    <link href="./zoom.css" rel="stylesheet">
    <script src="./zoom.js"></script>
    <script src="./transition.js"></script>
    <link rel="stylesheet" href="./style.css">

    <!-- favicon -->
    <link rel="shortcut icon" href="./img/ico.png">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=UA-129775907-1"></script> -->
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'UA-129775907-1');
    </script>

</head>

<body id="page-top" style="padding-top: 7em;">
    <nav class="navbar navbar-default navbar-fixed-top">
        <div class="container" style='max-width: 40rem; margin-left: auto; margin-right: auto;'>
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class='navbar-brand page-scroll' href='/'>
                    <span style='font-family:"Open Sans"; font-weight:300'>Kai Li</span>
                </a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <ul class="nav navbar-nav navbar-right">

                        <li style="font-size:20px"><a class='name' href="/">HOME</a></li>

                        <li style="font-size:20px"><a class='name' href="#publication">PUBLICATION</a></li>

                        <!-- <li style="font-size:20px"><a class='name' href="/recoder">RECODER</a></li> -->

                        <li style="font-size:20px"><a href="https://github.com/JusperLee"><i style="font-size:22px"
                                    class="fa fa-github"></i> GITHUB</a></li>
                        <!--<li style="font-size:20px"><a href="https://gitee.com/xinntao"><i style="font-size:22px"
                class="fa fa-github"></i> GITEE码云</a></li> -->
                        <li style="font-size:20px"><a href="https://scholar.google.com.hk/citations?user=fHkHcMsAAAAJ&hl=en"><i
                                    style="font-size:22px" class="ai ai-google-scholar"></i></a></li>
                    </ul>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container-fluid -->
    </nav>
    <div class='container'>
        <header class="masthead text-center">

            <!--=================Biography==========================-->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="30">
                <tr>
                    <td width="67%" valign="middle">
                        <p align="center">
                            <name>Hi There!</name>
                        </p>
                        <p align="left">
                            My name is Kai Li (Chinese name: 李凯). I'm is a first-year master student at Department of Computer Science and Technology, Tsinghua University, supervised by Prof. <a href="http://www.xlhu.cn/" target="_blank">Xiaolin Hu</a>.
                            I am also a member of <a href="https://ml.cs.tsinghua.edu.cn/" target="_blank">TSAIL Group</a> directed by Prof. <a href="https://www.cs.tsinghua.edu.cn/info/1121/3552.htm" target="_blank">Bo Zhang</a> and Prof. <a href="https://ml.cs.tsinghua.edu.cn/~jun/index.shtml"
                                target="_blank">Jun Zhu</a>. My research interests include super resolution, speech separation and cross-model speech separation.
                        </p>
                        <p align="left">
                            I was an intern at Tencent AI Lab, mainly doing research on causal speech separation, supervised by <a href="https://scholar.google.com/citations?user=OSM9xooAAAAJ&hl=en" target="_blank"> Yi Luo </a>.
                            <p align="left">
                                I got my bachelor's degree from <a href="https://cs.qhu.edu.cn/" target="_blank">Department of Computer
                                Technology and Application</a>, the Qinghai University, supervised by Prof. <a href="https://www.qhu-hdacp.cn/hjq.html" target="_blank">Jianqiang Huang</a> and Prof.
                                <a href="https://cs.qhu.edu.cn/jxgz/jxysz/ssyjsds/57658.htm" target="_blank">Chunmei Li</a> in 2020.
                            </p>
                            <p align="left" style="color:orange">
                                Notes <i class="fa fa-exclamation" aria-hidden="true" color="red"></i>
                            </p>
                            <ul style="list-style: none;">
                                <li>
                                    <p align="left" style="color:orange"><span class="glyphicon glyphicon-fire" style="color:red"></span> These works are open source to the best of my ability.
                                    </p>
                                </li>
                                <li>
                                    <p align="left" style="color:orange"><span class="glyphicon glyphicon-fire" style="color:red"></span> I am currently doing research on multimodal speech separation, and am interested in other speech tasks (e.g., pre-training models). If you would like
                                        to collaborate, please contact me. Many thanks.</p>
                        </p>
                        </li>
                        </ul>
                    </td>
                    <td width="33%">
                        <div style="width:1px; height:1px; visibility:hidden; overflow:hidden">
                            <img class='img-responsive center-block' src="./img/github.jpg" />
                        </div>
                        <img class='img-responsive center-block' src="./img/github.jpg" width="70%" height="70%" onmouseover="this.src='./img/github.jpg'" width="70%" height="70%" onmouseout="this.src='./img/github.jpg'" width="70%" height="70%" />
                    </td>
                </tr>
            </table>

            <div align="left" style="margin-top: -30px;margin-bottom: 30px;">
                &nbsp;&nbsp;<a href="https://github.com/JusperLee"><i style="font-size:26px" class="fa fa-github"></i>
                    <font size="4"> Github </font>
                    &nbsp;&nbsp;&nbsp;&nbsp;<a href="https://scholar.google.com.hk/citations?user=fHkHcMsAAAAJ&hl=en"><i
                            style="font-size:26px" class="ai ai-google-scholar"></i>
                        <font size="4"> Google Scholar </font>
                    </a> &nbsp;&nbsp;&nbsp;&nbsp;
                <a href="email.html"><i style="font-size:26px" class="fa fa-envelope-o"></i>
                        <font size="4"> Email </font>
                    </a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://twitter.com/KaiLi59332158"><i style="font-size:26px" class="fa fa-twitter"></i>
                        <font size="4"> Twitter </font>
                    </a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://www.zhihu.com/people/li-kai-34-50"><i style="font-size:26px"
                            class="fa fa-pencil-square-o"></i>
                        <font size="4"> 知乎 </font>
                    </a>
            </div>

            &nbsp;
            <div style="margin-top: -30px;margin-bottom: -30px;">
                <!-- border="1"-->
                <table width="10px" height="10px" align="center" border="1" cellspacing="0" cellpadding="0">
                    <tr>
                        <!--=======Audio-Visual-Datasets======-->
                        <td width="8%">
                            <a href="https://github.com/JusperLee/LRS3-For-Speech-Separation"><img style="height:50px;" src='./img/lrs3.png'></a>
                        </td>
                        <td align="left" valign="top" width="30%">
                            <paper-title>LRS3-For-Speech-Separation
                            </paper-title>
                            <div class=" paper-info">
                                Open source audio-visual dataset processing script.
                                <details>
                                    <summary>More</summary>
                                    Following are the steps to generate training and testing data. There are several parameters to change in order to match different purpose.
                                </details>
                            </div>
                            <span>[
                                <a
                                    href="https://github.com/JusperLee/LRS3-For-Speech-Separation"><strong>Github</strong></a>
                                ]&nbsp;<a href="https://github.com/JusperLee/Dual-Path-RNN-Pytorch/stargazers"><img
                                        alt="GitHub stars"
                                        src="https://img.shields.io/github/stars/JusperLee/LRS3-For-Speech-Separation?color=green&style=flat-square"
                                        style="border-radius: 0; margin: 0; display: inherit;"></a>
                            </span>
                        </td>
                        <!--=======dprnn======-->
                        <td width="8%">
                            <a href="https://github.com/JusperLee/Dual-Path-RNN-Pytorch"><img style="height:50px;" src='./img/dprnn.png'></a>
                        </td>
                        <td align="left" valign="top" width="30%">
                            <paper-title>DPRNN-Pytorch
                            </paper-title>
                            <div class=" paper-info">
                                Dual-path RNN
                                <details>
                                    <summary>More</summary>
                                    Efficient long sequence modeling for time-domain single-channel speech separation implemented by Pytorch.
                                </details>
                            </div>
                            <span>[
                                <a href="https://github.com/JusperLee/Dual-Path-RNN-Pytorch"><strong>Github</strong></a>
                                ]&nbsp;[
                                <a href="https://zhuanlan.zhihu.com/p/104606356"><strong>知乎:
                                        DPRNN阅读笔记</strong></a>]&nbsp;<a
                                    href="https://github.com/JusperLee/Dual-Path-RNN-Pytorch/stargazers"><img
                                        alt="GitHub stars"
                                        src="https://img.shields.io/github/stars/JusperLee/Dual-Path-RNN-Pytorch?color=green&style=flat-square"
                                        style="border-radius: 0; margin: 0; display: inherit;"></a>
                            </span>
                        </td>
                    </tr>

                    <tr>
                        <!--=======Look======-->
                        <td align="center" width="8%">
                            <a align="center" href="https://github.com/JusperLee/Looking-to-Listen-at-the-Cocktail-Party"><img style="height:50px;" src='./img/look.png'></a>
                        </td>
                        <td align="left" valign="top" width="30%">
                            <paper-title>Looking to Listen at the Cocktail Party
                            </paper-title>
                            <div class=" paper-info">
                                Audio-visual speech separation method :-)
                                <details>
                                    <summary>More</summary>
                                    The project is an audiovisual model reproduced by the contents of the paper Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation.
                                </details>
                            </div>
                            <span>[
                                <a
                                    href="https://github.com/JusperLee/Looking-to-Listen-at-the-Cocktail-Party"><strong>Github</strong></a>
                                ]&nbsp;[
                                <a href="https://zhuanlan.zhihu.com/p/86922308"><strong>知乎:
                                        LLCP阅读笔记</strong></a>]&nbsp;<a
                                    href="https://github.com/JusperLee/Looking-to-Listen-at-the-Cocktail-Party/stargazers"><img
                                        alt="GitHub stars"
                                        src="https://img.shields.io/github/stars/JusperLee/Looking-to-Listen-at-the-Cocktail-Party?color=green&style=flat-square"
                                        style="border-radius: 0; margin: 0; display: inherit;"></a>
                            </span>
                        </td>
                        <!--=======sdr======-->
                        <td align="center" width="8%">
                            <a href="https://github.com/JusperLee/Calculate-SNR-SDR"><img style="height:50px;" src='./img/sdr.png'></a>
                        </td>
                        <td align="left" valign="top" width="30%">
                            <paper-title>Calculate-SNR-SDR
                            </paper-title>
                            <div class=" paper-info">
                                Calculatie Audio‘s SNR and SDR.
                            </div>
                            <span>[
                                <a href="https://github.com/JusperLee/Calculate-SNR-SDR"><strong>GitHub</strong></a>
                                ]&nbsp;<a href="https://github.com/JusperLee/Calculate-SNR-SDR/stargazers"><img
                                        alt="GitHub stars"
                                        src="https://img.shields.io/github/stars/JusperLee/Calculate-SNR-SDR?color=green&style=flat-square"
                                        style="border-radius: 0; margin: 0; display: inherit;"></a>
                            </span>
                        </td>
                    </tr>

                    <tr>
                        <!--=======papers======-->
                        <td align="center" width="8%">
                            <a align="center" href="https://github.com/JusperLee/Speech-Separation-Paper-Tutorial"><img style="height:50px;" src='./img/tutorials.png'></a>
                        </td>
                        <td align="left" valign="top" width="30%">
                            <paper-title>Speech-Separation-Paper-Tutorial
                            </paper-title>
                            <div class=" paper-info">
                                A must-read paper and tutorial list for speech separation based on neural networks
                                <details>
                                    <summary>More</summary>
                                    None
                                </details>
                            </div>
                            <span>[
                                <a
                                    href="https://github.com/JusperLee/Speech-Separation-Paper-Tutorial"><strong>Github</strong></a>
                                ]&nbsp;<a
                                    href="https://github.com/JusperLee/Speech-Separation-Paper-Tutorial/stargazers"><img
                                        alt="GitHub stars"
                                        src="https://img.shields.io/github/stars/JusperLee/Speech-Separation-Paper-Tutorial?color=green&style=flat-square"
                                        style="border-radius: 0; margin: 0; display: inherit;"></a>
                            </span>
                        </td>
                        <!--=======convtasnet======-->
                        <td align="center" width="8%">
                            <a href="https://github.com/JusperLee/Conv-TasNet"><img style="height:50px;" src='./img/convtasnet.png'></a>
                        </td>
                        <td align="left" valign="top" width="30%">
                            <paper-title>Conv-TasNet
                            </paper-title>
                            <div class=" paper-info">
                                Conv-TasNet: Surpassing Ideal Time-Frequency Magnitude Masking for Speech Separation Pytorch's Implement
                            </div>
                            <span>[
                                <a href="https://github.com/JusperLee/Conv-TasNet"><strong>GitHub</strong></a> ]&nbsp;[
                                <a href="https://zhuanlan.zhihu.com/p/101235440"><strong>知乎:
                                        Conv-TasNet阅读笔记</strong></a>]&nbsp;<a
                                    href="https://github.com/JusperLee/Conv-TasNet/stargazers"><img alt="GitHub stars"
                                        src="https://img.shields.io/github/stars/JusperLee/Conv-TasNet?color=green&style=flat-square"
                                        style="border-radius: 0; margin: 0; display: inherit;"></a>
                            </span>
                        </td>
                    </tr>

                    <tr>
                        <!--=======upit======-->
                        <td align="center" width="8%">
                            <a align="center" href="https://github.com/JusperLee/UtterancePIT-Speech-Separation"><img style="height:50px;" src='./img/upit.png'></a>
                        </td>
                        <td align="left" valign="top" width="30%">
                            <paper-title>UtterancePIT
                            </paper-title>
                            <div class=" paper-info">
                                According to funcwj's uPIT, the training code supporting multi-gpu is written, and the Dataloader is reconstructed.
                                <details>
                                    <summary>More</summary>
                                    None
                                </details>
                            </div>
                            <span>[
                                <a
                                    href="https://github.com/JusperLee/UtterancePIT-Speech-Separation"><strong>Github</strong></a>
                                ]&nbsp;[
                                <a href="https://zhuanlan.zhihu.com/p/101232793"><strong>知乎:
                                        uPIT阅读笔记</strong></a>]&nbsp;<a
                                    href="https://github.com/JusperLee/UtterancePIT-Speech-Separation/stargazers"><img
                                        alt="GitHub stars"
                                        src="https://img.shields.io/github/stars/JusperLee/UtterancePIT-Speech-Separation?color=green&style=flat-square"
                                        style="border-radius: 0; margin: 0; display: inherit;"></a>
                            </span>
                        </td>
                        <!--=======convtasnet======-->
                        <td align="center" width="8%">
                            <a href="https://github.com/JusperLee/Deep-Clustering-for-Speech-Separation"><img style="height:50px;" src='./img/dpcl.png'></a>
                        </td>
                        <td align="left" valign="top" width="30%">
                            <paper-title>DPCL
                            </paper-title>
                            <div class=" paper-info">
                                Deep clustering in the field of speech separation implemented by pytorch
                            </div>
                            <span>[
                                <a
                                    href="https://github.com/JusperLee/Deep-Clustering-for-Speech-Separation"><strong>GitHub</strong></a>
                                ]&nbsp;[
                                <a href="https://zhuanlan.zhihu.com/p/101234149"><strong>知乎:
                                        DPCL阅读笔记</strong></a>]&nbsp;<a
                                    href="https://github.com/JusperLee/Deep-Clustering-for-Speech-Separation/stargazers"><img
                                        alt="GitHub stars"
                                        src="https://img.shields.io/github/stars/JusperLee/Deep-Clustering-for-Speech-Separation?color=green&style=flat-square"
                                        style="border-radius: 0; margin: 0; display: inherit;"></a>
                            </span>
                        </td>
                    </tr>
                    <tr>
                        <!--=======convtasnet======-->
                        <td align="center" width="8%">
                            <a href="https://github.com/JusperLee/AFRCNN-For-Speech-Separation"><img style="height:50px;" src='./img/afrcnn.png'></a>
                        </td>
                        <td align="left" valign="top" width="30%">
                            <paper-title>AFRCNN
                            </paper-title>
                            <div class=" paper-info">
                                Speech Separation Using an Asynchronous Fully Recurrent Convolutional Neural Network
                            </div>
                            <span>[
                                <a
                                    href="https://github.com/JusperLee/AFRCNN-For-Speech-Separation"><strong>GitHub</strong></a>
                                ]&nbsp;<a
                                    href="https://github.com/JusperLee/AFRCNN-For-Speech-Separation/stargazers"><img
                                        alt="GitHub stars"
                                        src="https://img.shields.io/github/stars/JusperLee/AFRCNN-For-Speech-Separation?color=green&style=flat-square"
                                        style="border-radius: 0; margin: 0; display: inherit;"></a>
                            </span>
                        </td>
                    </tr>
                </table>
            </div>


            <!--=================News==========================-->
            <h3 align="left">
                <a id="news"></a> <br>News
            </h3>
            <ul style="list-style: none;">
                <li><span class="glyphicon glyphicon-th-list"></span> [05/2022] 1 paper to appear in Interpseech 2022 !
                </li>
                <li><span class="glyphicon glyphicon-th-list"></span> [05/2022] 1 paper to submit in Nature Machine Intelligence. </a>
                </li>
                <li><span class="glyphicon glyphicon-th-list"></span> [03/2022] 1 paper to submit in IEEE Transactions on Industrial Informatics. </a>
                </li>

                <li><span class="glyphicon glyphicon-th-list"></span> [03/2022] 2 paper to submit in Interspeech 2022.
                    </a>

                </li>
                <li><span class="glyphicon glyphicon-th-list"></span> [10/2021] 1 paper to appear in NeurIPS 2021 </a> !
                </li>
                <li><span class="glyphicon glyphicon-th-list"></span> [05/2021] We won the 5% of the Global College Student Supercomputer Challenge (ASC20-21)</a> !
                </li>
                <li><span class="glyphicon glyphicon-th-list"></span> [01/2021] We won the global first prize of the <a href="http://www.asc-events.net/ASC20-21/Finals.php">2019
                        Global College Student Supercomputer Challenge (ASC20-21)</a> !
                </li>
                <li><span class="glyphicon glyphicon-th-list"></span> [06/2020] Outstanding Bachelor Thesis Award, Qinghai University of Computer Science and Technology !
                </li>
                <li><span class="glyphicon glyphicon-th-list"></span> [06/2020] Outstanding Graduates, Qinghai University of Computer Science and Technology !
                </li>
                <li><span class="glyphicon glyphicon-th-list"></span> [04/2020] 1 paper to appear in IET image processing.
                </li>
                <li><span class="glyphicon glyphicon-th-list"></span> [01/2020] I am an algorithm intern at Moyin Technology for speech dirazation and voiceprint recognition.
                </li>
                <details>
                    <summary>▶ Click for More</summary>
                    <li><span class="glyphicon glyphicon-th-list"></span> [11/2019] 1 paper to appear in ISPA2019.
                    </li>
                    <li><span class="glyphicon glyphicon-th-list"></span> [11/2019] We won the first prize of the first "Ganqingning" Innovation and Entrepreneurship Competition !
                    </li>
                    <li><span class="glyphicon glyphicon-th-list"></span> [11/2019] I won the National Scholarship, Ministry of Education, China !
                    </li>
                    <li><span class="glyphicon glyphicon-th-list"></span> [05/2019] We won the second prize in the Natural Academic Paper category of the National College Student Challenge Cup Qinghai Provincial Trial !
                    </li>
                    <li><span class="glyphicon glyphicon-th-list"></span> [05/2019] We won the first prize in the Qinghai Division of the 6th National Youth Science Innovation Experiment and Work Competition !
                    </li>
                    <li><span class="glyphicon glyphicon-th-list"></span> [05/2019] 1 papers to appear in ICDIP2019.
                    </li>
                    <li><span class="glyphicon glyphicon-th-list"></span> [04/2019] I won the second prize at the provincial level in the Blue Bridge Cup Java Group A!
                    </li>
                    <li><span class="glyphicon glyphicon-th-list"></span> [03/2019] We won the global second prize of the <a href="http://www.asc-events.net/ASC19/Preliminary.php">2019
                            Global College Student Supercomputer Challenge (ASC19)</a> !
                    </li>
                    <li><span class="glyphicon glyphicon-th-list"></span> [12/2018] We won the first prize of natural academic paper in the first "Principals Cup" Innovation and Entrepreneurship Competition in Qinghai Province !
                    </li>
                </details>
            </ul>



            <!--=================Publications==========================-->
            <h3 align="left">
                <a id="publication"></a> <br>Publications
            </h3>
            <!--<h4>2018</h4>-->

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
                <!--=======Audio visual======-->
                <tr>
                    <td width="35%">
                        <div class="paper-img"><img style="margin: auto;" align="middle" src='./img/submitted.png'>
                        </div>
                    </td>
                    <td align="left" valign="top" width="65%">
                        <paper-title><a href="#" target="_blank">An Auditory-Visual Speech Separation Model Inspired by
                                Cortico-thalamo-cortical Circuits</a>
                        </paper-title>
                        <div class=" paper-info">
                            <b>Kai Li</b>,
                            <a href="#" target="_blank">Fenghua Xie</a>,
                            <a href="#" target="_blank">Hang Chen</a>,
                            <a href="#" target="_blank">Kexin Yuan</a>,
                            <a href="#" target="_blank">Xiaolin Hu</a>,
                            <br>
                            <em>Submitted Nature Machine Intelligence</em>
                        </div>
                    </td>
                </tr>
                <!--=======super resolution======-->
                <tr>
                    <td width="35%">
                        <div class="paper-img"><img style="margin: auto;" align="middle" src='./img/cl-gan.png'>
                        </div>
                    </td>
                    <td align="left" valign="top" width="65%">
                        <paper-title><a href="#" target="_blank">Single Image Super-Resolution through Image Pixel
                                Information Clustering and Generative Adversarial Network</a>
                        </paper-title>
                        <div class=" paper-info">
                            <b>Kai Li</b>,
                            <a href="#" target="_blank">Jianqiang Huang</a>,
                            <a href="#" target="_blank">Lingbin Liu</a>,
                            <a href="#" target="_blank">Jinfang Jia</a>,
                            <a href="#" target="_blank">Yu Zhu</a>,
                            <a href="#" target="_blank">Li Wu</a>,
                            <a href="#" target="_blank">Xiaoying Wang</a>,
                            <br>
                            <em>Submitted IEEE Transactions on Industrial Informatics</em>
                        </div>
                    </td>
                </tr>
                <!--=======overss======-->
                <tr>
                    <td width="35%">
                        <div class="paper-img"><img style="margin: auto;" align="middle" src='./img/deep_mask.png'>
                        </div>
                    </td>
                    <td align="left" valign="top" width="65%">
                        <paper-title><a href="#" target="_blank">On the Use of Deep Mask Estimation Module for Neural
                                Source Separation Systems</a>
                        </paper-title>
                        <div class=" paper-info">
                            <b>Kai Li</b>,
                            <a href="http://www.xlhu.cn/" target="_blank">Xiaolin Hu</a>,
                            <a href="https://scholar.google.com/citations?user=OSM9xooAAAAJ&hl=en" target="_blank">Yi
                                Luo</a>,
                            <br>
                            <em>Interpseech 2022</em>
                        </div>
                        <span class=" block-text">
                            <a href="http://arxiv.org/pdf/2206.07347v1"
                                target="_blank"> <strong>&nbsp;Paper&nbsp;</strong></a></span>
                    </td>
                </tr>
                <!--=======joint_train======-->
                <tr>
                    <td width="35%">
                        <div class="paper-img"><img style="margin: auto;" align="middle" src='./img/causal.png'>
                        </div>
                    </td>
                    <td align="left" valign="top" width="65%">
                        <paper-title><a href="#" target="_blank">On the Design and Training Strategies for RNN-based
                                Online Neural Speech Separation Systems</a>
                        </paper-title>
                        <div class=" paper-info">
                            <b>Kai Li</b>,
                            <a href="https://scholar.google.com/citations?user=OSM9xooAAAAJ&hl=en" target="_blank">Yi
                                Luo</a>,
                            <br>
                            <em>Arxiv 2022</em>
                        </div>
                        <span class=" block-text">
                            <a href="http://arxiv.org/pdf/2206.07340v1"
                                target="_blank"> <strong>&nbsp;Paper&nbsp;</strong></a></span>
                    </td>
                </tr>
                <!--=======AFRCNN======-->
                <tr>
                    <td width="35%">
                        <div class="paper-img"><img style="margin: auto;" align="middle" src='./img/afrcnn.png'>
                        </div>
                    </td>
                    <td align="left" valign="top" width="65%">
                        <paper-title><a href="#" target="_blank"> Speech Separation Using an Asynchronous Fully
                                Recurrent Convolutional Neural Network</a>
                        </paper-title>
                        <div class=" paper-info">
                            <a href="#" target="_blank">Xiaolin Hu<sup>*, #</sup></a>,
                            <b>Kai Li<sup>*</sup></b>,
                            <a href="#" target="_blank">Weiyi Zhang</a>,
                            <a href="#" target="_blank">Yi Luo</a>,
                            <a href="#" target="_blank">Jean-Marie Lemercier</a>,
                            <a href="#" target="_blank">Timo Gerkmann</a>,
                            <br>
                            <em>NeurIPS 2021</em>
                        </div>
                        <span class=" block-text">
                            <a href="./project/AFRCNN" target="_blank"> <strong>&nbsp;Audio Demo
                                    Page&nbsp;</strong></a></span>
                        <span class=" block-text">
                            <a href="./project/AFRCNN-Enh" target="_blank"> <strong>&nbsp;Speech Enhancement
                                    Demo&nbsp;</strong></a></span>
                        <span class=" block-text">
                            <a href="https://github.com/JusperLee/AFRCNN-For-Speech-Separation" target="_blank">
                                <strong>&nbsp;Github Repository&nbsp;</strong></a></span>
                        <span class=" block-text">
                            <a href="https://papers.nips.cc/paper/2021/file/be1bc7997695495f756312886f566110-Paper.pdf"
                                target="_blank"> <strong>&nbsp;Paper&nbsp;</strong></a></span>
                    </td>
                </tr>
                <!--=======Bachelor Degree Thesis======-->
                <tr>
                    <td width="35%">
                        <div class="paper-img"><img style="" align="middle" src='./img/Bachelor-Degree-Thesis.png'>
                        </div>
                    </td>
                    <td align="left" valign="top" width="65%">
                        <paper-title><a href="#" target="_blank"> Research on Speech Separation Based on Audio Visual
                                Model</a>
                        </paper-title>
                        <div class=" paper-info">
                            <b>Kai Li</b>,
                            <a href="https://www.qhu-hdacp.cn/hjq.html" target="_blank">Jianqiang Huang</a>,
                            <a href="http://www.xlhu.cn/" target="_blank">Xiaolin Hu</a>
                            <br>
                            <em>Bachelor Degree Thesis, 2020</em>
                        </div>
                        <span class=" block-text">
                            <a href="./project/Pure-Audio/index.html" target="_blank"> <strong>&nbsp;Audio Project
                                    Page&nbsp;</strong></a></span>
                        <span class=" block-text">
                            <a href="./project/AV-Demo/AV-Model-Demo.html" target="_blank"> <strong>&nbsp;Audio-visual
                                    Project
                                    Page&nbsp;</strong></a></span>
                    </td>
                </tr>

                <!--=======A Survey of Single Image Super Resolution Reconstruction======-->
                <tr>
                    <td width="35%">
                        <div class="paper-img"><img style="" align="middle" src='./img/resolution_servey.png'>
                        </div>
                    </td>
                    <td align="left" valign="top" width="65%">
                        <paper-title>
                            <a href="#" target="_blank"> <b>A Survey</b> of Single Image Super Resolution Reconstruction
                            </a>
                        </paper-title>
                        <div class=" paper-info">
                            <b>Kai Li</b>, Shenghao Yang, Runting Dong,
                            <a href="https://www.qhu-hdacp.cn/hjq.html" target="_blank">Jianqiang Huang</a>,
                            <a href="https://www.qhu-hdacp.cn/wxy.html" target="_blank">Xiaoying Wang</a>
                            <br>
                            <em>IET Image Processing, 2020</em>
                        </div>
                        <span class=" block-text">
                            <a href="./files/A_Survey_of_Single_Image_Super_Resolution_Reconstr.pdf" target="_blank"> <strong>&nbsp;Paper&nbsp;</strong></a></span>
                    </td>
                </tr>
                <!--=======Single Image Super-resolution Reconstruction of Enhanced Loss Function with Multi-GPU Training======-->
                <tr>
                    <td width="35%">
                        <div class="paper-img"><img style="" align="middle" src='./img/lapras-GAN.png'>
                        </div>
                    </td>
                    <td align="left" valign="top" width="65%">
                        <paper-title><a href="#" target="_blank">Single Image Super-resolution Reconstruction of
                                Enhanced Loss
                                Function with Multi-GPU Training</a>
                        </paper-title>
                        <div class=" paper-info">
                            <a href="https://www.qhu-hdacp.cn/hjq.html" target="_blank">Jianqiang Huang<sup>*</sup></a>,
                            <b>Kai Li<sup>*</sup></b>,
                            <a href="https://www.qhu-hdacp.cn/wxy.html" target="_blank">Xiaoying Wang</a>
                            <br>
                            <em>Parallel & Distributed Processing with Applications(ISPA), 2019</em>
                        </div>
                        <span class=" block-text">
                            <a href="./files/Single_Image_Super-Resolution_Reconstruction_of_Enhanced_Loss_Function_with_Multi-GPU_Training.pdf" target="_blank"> <strong>&nbsp;Paper&nbsp;</strong></a></span>
                    </td>
                </tr>
                <!--=======Single image super resolution based on generative adversarial networks======-->
                <tr>
                    <td width="35%">
                        <div class="paper-img"><img style="" align="middle" src='./img/esrgan-pro.png'></div>
                    </td>
                    <td align="left" valign="top" width="65%">
                        <paper-title><a href="#" target="_blank">Single image super resolution based on generative
                                adversarial
                                networks</a>
                        </paper-title>
                        <div class=" paper-info">
                            <b>Kai Li</b>, Liang Ye, Shenghao Yang,
                            <a href="https://www.qhu-hdacp.cn/hjq.html" target="_blank">Jianqiang Huang</a>,
                            <a href="https://www.qhu-hdacp.cn/wxy.html" target="_blank">Xiaoying Wang</a>
                            <br>
                            <em>International Conference on Digital Image Processing (ICDIP), 2019</em>
                        </div>
                        <span class=" block-text">
                            <a href="./files/111790T.pdf" target="_blank"> <strong>&nbsp;Paper&nbsp;</strong></a></span>
                    </td>
                </tr>
                </td>
                </tr>
            </table>
            <p align="left">
                <small>(* equal contribution, <sup>#</sup> corresponding author)</small>
            </p>
            <div id="gitalk-container"></div>
            <p>
                <center>
                    <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=pGgE1hMket84twOks8hu76lWjl5ORukOJRPcVR9nRBI&co=2d78ad&ct=ffffff&cmo=3acc3a&cmn=ff5353'></script>

                    <br> &copy; Kai Li | Last updated: June 16st, 2022 | Theme by Xintao Wang
                </center>
            </p>
            <span style="display: block; margin-bottom: 3em"></span>
            </p>
        </header>
    </div>
    <script>
        const gitalk = new Gitalk({
            clientID: '25ba61997895f75e1270',
            clientSecret: '4cacd9b8a809f54f9b89cbc57c51077deb0c952a',
            repo: 'my-gittalk', // The repository of store comments,
            owner: 'JusperLee',
            admin: ['JusperLee'],
            distractionFreeMode: true, // Facebook-like distraction free mode
            id: window.location.pathname
        })
        gitalk.render('gitalk-container')
    </script>
</body>

</html>

</html>